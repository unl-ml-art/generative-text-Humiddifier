{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7LoMj4GA4n_"
   },
   "source": [
    "#  GPT-2 Generation and Fine-Tuning\n",
    "\n",
    "This notebook explores GPT-2 (Generative Pretrained Transformer-2) from OpenAI. Read more about it [here](https://openai.com/blog/better-language-models/).\n",
    "\n",
    "Activities include:\n",
    "\n",
    "0. Setup\n",
    "1. Generate samples from pre-trained gpt-3 model\n",
    "2. Fine-tune gpt-2 on text of your choosing. \n",
    "\n",
    "Adapted by Robert Twomey (rtwomey@unl.edu) for Machine Learning for the Arts SP22 from this [google colab](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) by [Max Woolf](http://minimaxir.com). See his repo [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run once to install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q gpt-2-simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart the kernel and run the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KBkpRgBCBS2_"
   },
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj2IJLHP3KwE"
   },
   "source": [
    "## GPU\n",
    "\n",
    "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
    "\n",
    "You can verify which GPU is active by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUmTooTW3osf",
    "outputId": "c9fcfa4f-277d-4b3e-8974-373066dc157b"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the memory usage (0MiB / 32510MiB) for the Tesla V100.\n",
    "You can re-rerun the above cell to see what memory your code/models are using during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wXB05bPDYxS"
   },
   "source": [
    "## Downloading GPT-2\n",
    "\n",
    "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
    "\n",
    "There are three released sizes of GPT-2:\n",
    "\n",
    "* `124M` (default): the \"small\" model, 500MB on disk.\n",
    "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
    "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
    "* `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
    "\n",
    "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
    "\n",
    "The next cell downloads it from Google Cloud Storage and saves it in the the current working directory at `/models/<model_name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8wSlgXoDPCR",
    "outputId": "10fc0d7c-d18f-4e11-a2af-bfade8b537eb"
   },
   "outputs": [],
   "source": [
    "model_name = \"124M\" # largest model we can fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run once to download the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.download_gpt2(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQAN3M6RT7Kj"
   },
   "source": [
    "# 1. Generate Text From The Pretrained Model\n",
    "\n",
    "If you want to generate text from the pretrained model pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`. (This is currently the only way to generate text from the 774M or 1558M models with this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "BAe4NpKNUj2C",
    "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae"
   },
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.load_gpt2(sess, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the model\n",
    "The follow cell samples from gpt-2, using the provided prefix (seed) and other parameters. It starts the TF session and generates the samples.\n",
    "\n",
    "Try changing the parameters below to change the output: \n",
    "- `prefix` is the prompt. This will be the starting string/seed for your generation. Use your own text. \n",
    "- `temperature` sets the variability/randomness of the output. Range 0.0-1.0\n",
    "- `length` sets the lenght of output (in tokens). max is 1024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "-xInIZKaU104",
    "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d"
   },
   "outputs": [],
   "source": [
    "gpt2.generate(sess,\n",
    "              model_name=model_name,\n",
    "              prefix=\"There is a pop song playing\",\n",
    "              length=100,\n",
    "              temperature=0.7,\n",
    "              top_p=0.9,\n",
    "              nsamples=5,\n",
    "              batch_size=5\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities\n",
    "- try varying the prefix. \n",
    "  - what length of prefix works best with the given model? \n",
    "  - how does the choice of prefix change the format/form of the output.\n",
    "- try varying the temperature.\n",
    "- try loading the different sized models (124M, 355M, 774M, 1558M) and generate text without changing the other parameters. \n",
    "  - Do you notice any qualitative differences in the output? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-Tuning GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already generated with gpt2, you need to reset the tf graph and gpt2 session. Otherwise, we create a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeXshJM-Cuaf",
    "outputId": "a3c75caa-917b-4818-ca2d-d78610d8b6f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 11:57:46.597314: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-10 11:57:47.336241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:5e:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"355M\" # same model as selected above\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# check if sess exists (e.g. if we ran section 1 above)\n",
    "var_exists = 'sess' in locals() or 'sess' in globals()\n",
    "\n",
    "if not var_exists:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeeSKtNWUedE"
   },
   "source": [
    "## Upload a text file\n",
    "For this, we will use a text file you provide to finetune (continue training) GPT-2. You can use any plain text (.txt) file. \n",
    "\n",
    "Simply drag and dropy our text file into the file browser at left. \n",
    "\n",
    "Once you have uploaded your file, update the file name in the cell below, then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6OFnPCLADfll"
   },
   "outputs": [],
   "source": [
    "file_name = \"BeanieBabyPoems_NoGerman.txt\" # your file here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdpZQXknFNY3"
   },
   "source": [
    "## Run the finetuning\n",
    "\n",
    "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
    "\n",
    "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every `save_every` steps (can be changed) and when the cell is stopped.\n",
    "\n",
    "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them. If your input text is smaller, training might proceed more quickly.\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
    "\n",
    "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
    "* **`sample_every`**: Number of steps to print example output\n",
    "* **`print_every`**: Number of steps to print training progress.\n",
    "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
    "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
    "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 11:57:59.897652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:5e:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4266.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 8804 tokens\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 | 10.46] loss=3.06 avg=3.06\n",
      "[20 | 18.09] loss=2.98 avg=3.02\n",
      "[30 | 25.72] loss=2.88 avg=2.97\n",
      "[40 | 33.34] loss=2.65 avg=2.89\n",
      "[50 | 40.97] loss=2.54 avg=2.82\n",
      "[60 | 48.59] loss=2.51 avg=2.77\n",
      "[70 | 56.25] loss=2.25 avg=2.69\n",
      "[80 | 63.87] loss=2.21 avg=2.63\n",
      "[90 | 71.48] loss=1.98 avg=2.55\n",
      "[100 | 79.12] loss=2.00 avg=2.50\n",
      "======== SAMPLE 1 ========\n",
      "\n",
      "To say that people who have never even been born have yet been born is an understatement.\n",
      "\n",
      "It is an amazing story that I will share with you in \"How I Made the Decision to Not Be a Baby\"\n",
      "\n",
      "My mother never knew what we did for fun, we were never allowed to have fun, we were always afraid\n",
      "\n",
      "Then when her beautiful face became quite red she gave me this magic toy\n",
      "\n",
      "Now you can be the one who can take that toy out with you!\n",
      "\n",
      "\n",
      "Tears have been very prominent on my face all summer long.\n",
      "\n",
      "We have so many things to say over the years, this will give you a rare insight into my little soul\n",
      "\n",
      "What I have been through in the past will be remembered.\n",
      "\n",
      "After a break from our home, we came back here all the while\n",
      "\n",
      "But it isn't until we make dinner that we see\n",
      "Because there is always a delicious dish to be had!\n",
      "\n",
      "\n",
      "I am your friend. You will not have a hard time coming to me\n",
      "What you have got for your birthday is just as important as it is for you\n",
      "But you're not my friend because you have always been wrong\n",
      "\n",
      "\n",
      "Tears have been very prominent on my face all summer long.\n",
      "\n",
      "We have so many things to say over the years, this will give you a rare insight into my little soul\n",
      "What I have been through in the past will be remembered.\n",
      "\n",
      "After a break from our home, we came back here all the while\n",
      "But it isn't until we make dinner that we see\n",
      "\n",
      "Because there is always a delicious dish to be found!\n",
      "\n",
      "\n",
      "Our house is a beautiful paradise for people in all ages.\n",
      "\n",
      "Every day we want to be happy as we go,\n",
      "What has gotten us through so much?\n",
      "\n",
      "\n",
      "In the days you don't have to worry about your clothes, the dress you would like.\n",
      "Look at this beautiful collection. I love that little dress!\n",
      "\n",
      "\n",
      "In the days when we used to play sports, there was always a game,\n",
      "We were used to it, there was no one to beat us!\n",
      "\n",
      "\n",
      "After spending a night with my friend, you will like my bed,\n",
      "I always do my best to wake you up from your dream.\n",
      "\n",
      "\n",
      "There is no doubt that each day counts for something good,\n",
      "Every day you will have a little treat that you will share.\n",
      "\n",
      "\n",
      "Your bed is perfect because every bed you sleep in is a treat for you.\n",
      "\n",
      "\n",
      "Every night dreams were made of gold, but they are true today.\n",
      "Your bed is also precious because everyday, you will be lucky to find a piece.\n",
      "\n",
      "\n",
      "It is one of the most beautiful times in your life. I wish you so much when you begin the rest.\n",
      "\n",
      "\n",
      "I am so glad you are here with me, I don't know what will happen, I know I have something tough to say.\n",
      "\n",
      "\n",
      "Your bed is for me, every night I dream of you,\n",
      "Every room needs a little sleep, every dream is mine!\n",
      "\n",
      "\n",
      "I will not hold back when I cry though, I can't wait to make you into my baby.\n",
      "\n",
      "\n",
      "Your bed is my best friend, she has nothing to fear from you, just be happy.\n",
      "\n",
      "\n",
      "Every night when I am out, I am with you, I'm with you even though you are away.\n",
      "Every night you will hear my song every night,\n",
      "\"Oh, my baby, oh, oh my baby, oh, oh\"\n",
      "\n",
      "\n",
      "I will make sure your room is ready for you when you come to bed, I will come pick you up and give you a hug when you are asleep.\n",
      "\n",
      "\n",
      "Every day is a new day for me, I will wake you up in the morning without a care in the world.\n",
      "\n",
      "\n",
      "Every day I am sure you will be a part of my family, there is no telling how you will feel.\n",
      "\n",
      "\n",
      "Every night I make sure you stay strong, my bed is made with love in the ground.\n",
      "\n",
      "\n",
      "I am so glad you are with me, I am so glad you are here with me.\n",
      "\n",
      "\n",
      "Every night after so many nights I want you to know how good I made you.\n",
      "\n",
      "\n",
      "Every night makes you smile that you don't always see that you had a dream.\n",
      "\n",
      "\n",
      "Your dreams make you cry too, when you go to bed do you cry?\n",
      "\n",
      "\n",
      "Every night I give you a hug and you can't stop, I will put my hand on your cheek and make sure you get a good night's rest.\n",
      "\n",
      "\n",
      "Every night I want to please you, I can't help but love you, every time I give you a kiss, you will wish I was here too.\n",
      "\n",
      "\n",
      "Every night I will always remember you, every time you see me, you seem so sweet, I always feel so lucky, never seen anything this good.\n",
      "\n",
      "\n",
      "Every night as you wake up, I am sure you\n",
      "\n",
      "[110 | 94.06] loss=1.61 avg=2.41\n",
      "[120 | 101.70] loss=1.45 avg=2.33\n",
      "[130 | 109.33] loss=1.27 avg=2.24\n",
      "[140 | 116.95] loss=1.28 avg=2.17\n",
      "[150 | 124.57] loss=0.84 avg=2.07\n",
      "[160 | 132.21] loss=0.67 avg=1.98\n",
      "[170 | 139.83] loss=0.65 avg=1.89\n",
      "[180 | 147.47] loss=0.59 avg=1.81\n",
      "[190 | 155.10] loss=0.48 avg=1.74\n",
      "[200 | 162.74] loss=0.40 avg=1.66\n",
      "Saving checkpoint/run1/model-200\n",
      "======== SAMPLE 1 ========\n",
      " great, he'll make you happy.\n",
      "The sun is just up in the sky.\n",
      "Happy and proud of him, don't let him down\n",
      "He's smart as a rat and has a plan!\n",
      "And now for the Temptations...\n",
      "A band of thieves has been sighted snooping around!\n",
      "Looks like this band of thieves might\n",
      "You bet!\n",
      "\n",
      "\n",
      "Happiness is a gift from the Gods\n",
      "A people full of happiness,\n",
      "Valor and the sun is their favorite thing!\n",
      "Pleasure is the number one virtue in the land!\n",
      "Believe me, if you had to pick one,\n",
      "My heart would say `happy should be number one!'\n",
      "\n",
      "\n",
      "Hippie the mouse is a favorite mouse friend\n",
      "Hippie loves to play fetch\n",
      "She loves to read and to write.\n",
      "Hippie loves so much to the tree she lived\n",
      "Hippie is sure to now rest easy knowing!\n",
      "\n",
      "\n",
      "Inch the tree on fire is an unusual idea\n",
      "It seems as though the wind has been blowing\n",
      "But in fact, Inch is rather peaceful.\n",
      "She laughs all night at your jokes and songs!\n",
      "Believe me, she'll stay like this for ever!\n",
      "\n",
      "\n",
      "Jungle the bobcat hides in the night\n",
      "Underneath the starslight,\n",
      "He doesn't roam out of the night\n",
      "He will sometimes sneak in and out very late\n",
      "Looking for rain, his coat is as yellow as can be!\n",
      "If he doesn't find him, then so be it!\n",
      "\n",
      "\n",
      "Kiwi the tarantula bites when not in a kiddie mood\n",
      "It's a pity that this little spider doesn't come out often\n",
      "This kiddie treat is really quite tasty!\n",
      "If you do, he'll come out and bite!\n",
      "\n",
      "\n",
      "Knuckles the cheetah is a bit of a cheetah\n",
      "Although it loves to chew,\n",
      "Knuckles is really quite the cheetamot!\n",
      "If you do, then you might as be sure\n",
      "That you have Jelly Belly Syndrome!\n",
      "\n",
      "\n",
      "Loewe the puffiwuffet\n",
      "After spending most of your day curled up\n",
      "Loewe waits for you patiently\n",
      "Snoozing and reading in the dark for you!\n",
      "I'm so glad you found me!\n",
      "\n",
      "\n",
      "Milky the puffiwuffet\n",
      "While walking along busy streets\n",
      "No one will ever find out\n",
      "Millin' and chewing his nails,\n",
      "Walking right along with no excuse.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Negg the puffiwuffet\n",
      "Negg stands out from the crowd\n",
      "Even though he looks like a moron\n",
      "He still gets his runs right here\n",
      "Because he's got a ton of left over cash\n",
      "Walk him back to school, or the way he's dressed!\n",
      "\n",
      "\n",
      "Nanook the puffiwuffet\n",
      "It's been almost six in the morning and now I'm dreaming\n",
      "Nanook likes to nap in the sun in the trees\n",
      "Over the winter months she like to dance\n",
      "Nanook's a lucky beanie winner!\n",
      "\n",
      "\n",
      "Nota the parrot knows what time of year\n",
      "She likes to hop between trees and wade through\n",
      "Playing in the shade of a spring tree\n",
      "That way she'll let you in on a secret!\n",
      "\n",
      "\n",
      "Pinky the puffiwuffet\n",
      "She's a sweet bunny who just wants to be\n",
      "Not much is known about her other half\n",
      "But she's sure to know that there's nothing to be afraid\n",
      "That's why she's been so friendly and friendly\n",
      "As she walks she swats the water she seems\n",
      "Nibbling really makes her happy!\n",
      "\n",
      "\n",
      "Radar the puffiwuffet\n",
      "Bright and sunny in the day\n",
      "Even though Radar is wingless\n",
      "She's so kind and kind throughout the day\n",
      "All that is blue and pink and pink\n",
      "That's her theory how everything is!\n",
      "\n",
      "\n",
      "Rat the owl on a hot day\n",
      "She acts really calm and collected\n",
      "After all, this is the sort of bird you'll be\n",
      "If you tried to approach her, she'd just say\n",
      "You know what? You'd be a little crazy!\n",
      "\n",
      "\n",
      "Radar the parrot on the beach\n",
      "She acts really nice and kind\n",
      "But really she's just a sweet and kind bird\n",
      "\n",
      "Because she's so kind and kind is what she is\n",
      "You know? She's a sweet and kind bird!\n",
      "\n",
      "\n",
      "Roam the black parrot on the fire beach\n",
      "Over the fire and behind the waves\n",
      "Their bellies are huge and big warm\n",
      "So Roam would be as kind and kind of you and me\n",
      "You and I would be friends for a long time to come.\n",
      "\n",
      "<|endoftext|>First, lets speak about the game.\n",
      "\n",
      "At E3 this week, we had the privilege of witnessing the game from the very beginning\n",
      "\n",
      "First, we noticed the game had been made by hand - and that one thing was very pretty\n",
      "this hand was red! And what a hand\n",
      "\n",
      "[210 | 178.47] loss=0.36 avg=1.59\n",
      "[220 | 186.09] loss=0.19 avg=1.52\n",
      "[230 | 193.73] loss=0.15 avg=1.46\n",
      "[240 | 201.35] loss=0.22 avg=1.40\n",
      "[250 | 208.97] loss=0.13 avg=1.34\n",
      "[260 | 216.60] loss=0.12 avg=1.29\n",
      "[270 | 224.23] loss=0.10 avg=1.24\n",
      "[280 | 231.85] loss=0.15 avg=1.19\n",
      "[290 | 239.48] loss=0.08 avg=1.15\n",
      "[300 | 247.10] loss=0.09 avg=1.11\n",
      "======== SAMPLE 1 ========\n",
      "\n",
      "Amber's Eyes\n",
      "\n",
      "Amber\n",
      "\n",
      "Amber's Eyes\n",
      "\n",
      "Pretty\n",
      "Bright\n",
      "Bright\n",
      "\n",
      "From\n",
      "South\n",
      "Southwest\n",
      "To\n",
      "Hawaii\n",
      "Mel\n",
      "Halo\n",
      "her Eyes\n",
      "Way\n",
      "back\n",
      "Ally\n",
      "How\n",
      "To\n",
      "Shake it Off\n",
      "I'm\n",
      "Earsie\n",
      "is\n",
      "doing\n",
      "her\n",
      "obscure.\n",
      "\n",
      "\n",
      "Aubrey the Tank Girl\n",
      "\n",
      "Pink\n",
      "\n",
      "comes with a\n",
      "skull\n",
      "and a lamp\n",
      "all otherows\n",
      "girls of\n",
      "Pink\n",
      "stockinette\n",
      "garment.\n",
      "\n",
      "\n",
      "Aubrey the Unicorn\n",
      "\n",
      "Unicorn\n",
      "\n",
      "twas sexy\n",
      "as any unicorn.\n",
      "twice as\n",
      "tall\n",
      "you'll spot\n",
      "twenty of them.\n",
      "\n",
      "\n",
      "Aurora the Mountain Lion\n",
      "\n",
      "Beautiful and sunny\n",
      "don't be afraid\n",
      "of a lion's attack\n",
      "ten feet from here.\n",
      "\n",
      "\n",
      "Antelopes the Bull\n",
      "\n",
      "This is the beast that hides in the ground\n",
      "Aceroid Oxytocin Boosters\n",
      "He's very strong and quick\n",
      "he even defeated one of these\n",
      "bulls.\n",
      "\n",
      "\n",
      "Antigua Barbados\n",
      "\n",
      "Bust a boa in the bud\n",
      "She likes it in this short video gallery.\n",
      "\n",
      "\n",
      "Antelope the Gold Bear\n",
      "\n",
      "He's been holding classes for the day\n",
      "goat likes to watch and eat\n",
      "\n",
      "\n",
      "Atshos the Gold Bear\n",
      "\n",
      "Black and gold, this bear is seen\n",
      "tinkering by the 100 jugglers\n",
      "every night.\n",
      "\n",
      "\n",
      "Andrea the Legit Bear\n",
      "\n",
      "Legit growth has her eyes sparkling\n",
      "She needs lots of sun, lots of\n",
      "she'll get plenty of sun with this bear\n",
      "day to come.\n",
      "\n",
      "\n",
      "Andrea the Pit Bull\n",
      "\n",
      "She's healthy and she's recovering\n",
      "She loves to play basketball\n",
      "She is sure to win the basketball\n",
      "it's said a great deal by way of\n",
      "dance music.\n",
      "\n",
      "\n",
      "Andrea the Red Bear\n",
      "\n",
      "She's been trying all day to grow\n",
      "she's been fighting pain in her legs\n",
      "until the bear gave in and give more\n",
      "she could rock the boat!\n",
      "\n",
      "\n",
      "Andrea's Day Bunny\n",
      "\n",
      "Caribbean moms make sure their babies get enough\n",
      "daytime days are plenty sunshine and rain\n",
      "this bear is the one to give her a call\n",
      "she always receives lots of compliments!\n",
      "\n",
      "\n",
      "Andrea's Day Trio:\n",
      "\n",
      "Long and lean, her back to the ball\n",
      "Aerial and dartboard friendly,\n",
      "Each with their own unique play style,\n",
      "Bring your own snack or treat!\n",
      "\n",
      "\n",
      "Andrea the Pit Bull\n",
      "\n",
      "She's trying her hand at a name,\n",
      "Before she fell in love with Carson\n",
      "She gave birth to their child, February!\n",
      "\n",
      "\n",
      "Andrea the Queen Bear\n",
      "\n",
      "Twitching her nose for lip gloss,\n",
      "Shoving it in her hair, it makes her look so,\n",
      "Quite so, isn't she?\n",
      "\n",
      "\n",
      "Andrea the Unicorn\n",
      "\n",
      "Her hair is golden in the sun,\n",
      "Like a star on her cap\n",
      "She's been searching for her love for a while.\n",
      "\n",
      "\n",
      "Andrea the White Bear\n",
      "\n",
      "Her hair is silken and silky smooth\n",
      "Her eyes are open and clear,\n",
      "Like a spring leaf her way around the house\n",
      "Every day is wonderful and kind!\n",
      "\n",
      "\n",
      "Andrea the Snow Leopard\n",
      "\n",
      "Andrea the Leopard is a delicious smile\n",
      "Remembering that she's very brave\n",
      "When In Love, You Knew Nothing, WEREN'T!\n",
      "\n",
      "\n",
      "Andrea the Texaco Bear\n",
      "\n",
      "This bear is the Texaco's way cute\n",
      "She's been keeping up the good work\n",
      "Keep her out of trouble!\n",
      "\n",
      "\n",
      "Andrea the Tiger\n",
      "\n",
      "Tipping her nose and taking a swig\n",
      "Looking mighty popular\n",
      "With the tiger, she gets lots of laughs\n",
      "Peeling off her clothes and handing out pamphlets\n",
      "She's the one giving it to theoth rave!\n",
      "\n",
      "\n",
      "Andrea the Stroganie Bliley\n",
      "\n",
      "This little bliley is called a widow's bloke\n",
      "She said a very nice joke and oohed and a augh was hit up\n",
      "Things are looking quite a good for after all!\n",
      "\n",
      "\n",
      "Andrea the Tie-Dyed Bear\n",
      "\n",
      "Andrea the Tie-Dyed Bear loves to sew\n",
      "She always has so many wonderful patterns\n",
      "She's a dream come true and a master at it\n",
      "Making her a darlings in her own unique art!\n",
      "\n",
      "\n",
      "Andrea the Tie-In Bear\n",
      "\n",
      "She has a sparkly face and a sweet touch\n",
      "Just the way she is, a star to anyone who sees\n",
      "Making it a goal she will make it!\n",
      "\n",
      "\n",
      "Andrea the Teal Dot\n",
      "\n",
      "A lilac eye line for you to see\n",
      "If you look closely, you will see that\n",
      "Teal colors are what make her so special!\n",
      "\n",
      "\n",
      "Andrea the Teal Dot Baby\n",
      "\n",
      "\n",
      "\n",
      "[310 | 261.21] loss=0.08 avg=1.07\n",
      "[320 | 268.85] loss=0.11 avg=1.04\n",
      "[330 | 276.47] loss=0.08 avg=1.00\n",
      "[340 | 284.10] loss=0.13 avg=0.97\n",
      "[350 | 291.74] loss=0.09 avg=0.94\n",
      "[360 | 299.36] loss=0.07 avg=0.91\n",
      "[370 | 306.98] loss=0.09 avg=0.89\n",
      "[380 | 314.61] loss=0.07 avg=0.86\n",
      "[390 | 322.25] loss=0.08 avg=0.84\n",
      "[400 | 329.88] loss=0.07 avg=0.81\n",
      "Saving checkpoint/run1/model-400\n",
      "WARNING:tensorflow:From /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/training/saver.py:969: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "======== SAMPLE 1 ========\n",
      " the more you play\n",
      "The harder you play the more money you earn! (Loosy laugh)\n",
      "Win or lose the Civil War it was a close one! (Chorus)\n",
      "Win or lose in heaven or hell! (Hair Loops)\n",
      "Hope you found this trip a lot of fun and expense\n",
      "Let me know how you did, if you didn't!\n",
      "\n",
      "\n",
      "Spinner the Spider\n",
      "\n",
      "This spinner is as small or smaller than you!\n",
      "It'll take you a while to get it down!\n",
      "Be careful out there, for the one that eats me,\n",
      "It can eat you up fast!\n",
      "\n",
      "\n",
      "Squealer the Red Spot\n",
      "\n",
      "Squealer's ears are small,\n",
      "It's a guessing game, the answer is a--\n",
      "It's a long way from here, but it's a goal in heart\n",
      "Stay close, for the love of love,\n",
      "I can guarantee you we'll see--\n",
      "\n",
      "\n",
      "Stilts the Stooge\n",
      "\n",
      "Hands up/ is he pointing down\n",
      "ie saying \"Hi,\"/he's pointing straight at\n",
      "This goose acts a little crazy by the dozen!\n",
      "\n",
      "(olive show on)\n",
      "\"Ugh...\"\n",
      "Is this how you are?\n",
      "Listen closely, listen to me\n",
      "There's no need to be so cryptic\n",
      "My purpose is simple, for I know\n",
      "That all children will love and adored!\n",
      "\n",
      "\n",
      "Strawberry the Ocotillo\n",
      "\n",
      "I love this olives and this strawberry\n",
      "What's the olives for you, sweetheart?\n",
      "Get lost and nothing will entertain\n",
      "This ocotillo feels guilty for stealing\n",
      "What's the deal? You're the one who bought me a piece\n",
      "Cocktail it is I who receive it!\n",
      "\n",
      "\n",
      "Scoop the Pelican\n",
      "\n",
      "The boat is stowed, the sinker low\n",
      "Couldn't reach the deck while Pelican was sinking\n",
      "I'll show you a way!/\n",
      "(to Ocotpie)\n",
      "\"Can you believe I am able to stow the ocelot?\n",
      "The Pelican that lays on the bottom of the Lake Michigan is lucky\n",
      "One that can't seem to find a spot!\n",
      "\n",
      "\n",
      "Scottie the Terrier\n",
      "\n",
      "Scottie is a little periwinkle shy\n",
      "Taking the time to love is a hobby\n",
      "So don't take my word for it,\n",
      "If anyone would like to donate, just hit the \"C\" at the top\n",
      "The money will go to help charity!\n",
      "\n",
      "\n",
      "Scotty the Terrier\n",
      "\n",
      "Is he Theo the Terrier?/oh my! he is so sweethe loves the fruit\n",
      "And loves to eat it! so please know\n",
      "\n",
      "\n",
      "Secret Santa\n",
      "\n",
      "Seeking a secret santa,\n",
      "Methodical and methodical enough\n",
      "As she goes, she comes out stronger than you\n",
      "A present for you and me, a gift for the darlings\n",
      "Set for you and me, this item will be a must have\n",
      "Ringo will be the first to show!\n",
      "\n",
      "\n",
      "Sting the Stingray\n",
      "\n",
      "I'm a geiger counter, my name is Sting\n",
      "I'm the Stingray, you're the know-it-all\n",
      "I'm the bird that everyone knows, long live the golden rum\n",
      "Living in darkness.\n",
      "\n",
      "\n",
      "Scotty the Terrier\n",
      "\n",
      "Remember how you lived your happiest days\n",
      "As a geiger counter, remember how\n",
      "writing is about as sweet as it's nice\n",
      "Learning by doing, reading is a great way\n",
      "Using logic to your advantage, says friend--\n",
      "\"I get in a rhythm, will you join me?\"\n",
      "\n",
      "\n",
      "Snipes the Seahorse\n",
      "\n",
      "Snipe it, don't eat my green salad\n",
      "This is a vegetable, it's about time I got\n",
      "My money, my life, and my eggs brown\n",
      "Ringing the song of the oceans, \"I feel like a star\"\n",
      "\n",
      "\n",
      "Snow the Bear\n",
      "\n",
      "I am a bear, I am one\n",
      "I am snow, I am a flower\n",
      "I keep my eyes shut, I am safe\n",
      "I am someone else if need be\n",
      "I get down, I get up, I'm so down\n",
      "ive been hibernating for the night\n",
      "this is how I met me: a hibernating bunny\n",
      "\n",
      "\n",
      "Sting the Stingray\n",
      "\n",
      "I'm a tempa, I'm stuttering, I'm slow\n",
      "I want to snooze, I want to know\n",
      "is it cold inside, is it in the late fall or winter\n",
      "do you have any tips for a stutter bite?\n",
      "\n",
      "\n",
      "Stinger the Panda\n",
      "\n",
      "Panda cub, you've got to grow a stand up\n",
      "this little panda cub will blow your mind\n",
      "he is the one who taught you how to stand\n",
      "he taught you so much, he also did not give you a toy.\n",
      "\n",
      "\n",
      "Tank the Panther\n",
      "\n",
      "This little panda, it's quite a sight\n",
      "to humans it's really\n",
      "\n",
      "[410 | 345.28] loss=0.09 avg=0.79\n",
      "[420 | 352.91] loss=0.07 avg=0.77\n",
      "[430 | 360.53] loss=0.06 avg=0.75\n",
      "[440 | 368.15] loss=0.07 avg=0.73\n",
      "[450 | 375.78] loss=0.05 avg=0.71\n",
      "[460 | 383.41] loss=0.05 avg=0.70\n",
      "[470 | 391.03] loss=0.07 avg=0.68\n",
      "[480 | 398.66] loss=0.05 avg=0.66\n",
      "[490 | 406.29] loss=0.05 avg=0.65\n",
      "[500 | 413.97] loss=0.05 avg=0.63\n",
      "======== SAMPLE 1 ========\n",
      "ent\n",
      "\n",
      "Once upon a time in a land far away,\n",
      "A beautiful girl used to visit,\n",
      "A beacon of hope to her children,\n",
      "Dreamt of and trusted, is she,\n",
      "On this land.\n",
      "\n",
      "\n",
      "Canyon Cowboy\n",
      "\n",
      "One day driving down a desolate road,\n",
      "Drive by and see what he's made of!\n",
      "It's a funny thing, he sees a bunch,\n",
      "Tempt him a hundred and one, even better!\n",
      "\n",
      "\n",
      "chihuahua\n",
      "\n",
      "The chihuahua world is a little\n",
      "Blizzard is coming up south,\n",
      "Remember to stay out of the way!\n",
      "\n",
      "\n",
      "Chip the Chipmunk\n",
      "\n",
      "Chip the chipmunk's a tough nut to crack\n",
      "Find him a toy that will last\n",
      "Keep him around for as long as you can\n",
      "Chip has this toy called a star\n",
      "That means a billion!\n",
      "\n",
      "\n",
      "Cock in a Hat\n",
      "\n",
      "Once upon a time in a land far away\n",
      "A cock that size, can't be held\n",
      "\n",
      "A cock that's a mess. But the most cunning,\n",
      "Every command he gets, from a master that he is\n",
      "Every bit the guy you think won a fight!\n",
      "\n",
      "\n",
      "Crunch the Parrot\n",
      "\n",
      "Some days is pretty bad for sleep,\n",
      "Many a time a rough patch gets in the air\n",
      "Some the time, like this parrot,\n",
      "Sometimes it's hard to see after a hard day's a hard\n",
      "Winter.\n",
      "\n",
      "\n",
      "Coral the Fish\n",
      "\n",
      "Coral is a beautiful country\n",
      "Swinging in the sea, bass and rays\n",
      "Ate flying colors well, some do\n",
      "Some, but not all, of these fish like this\n",
      "One. Stick. My. TENSE.\n",
      "\n",
      "\n",
      "Crunch the Shark\n",
      "\n",
      "Crunch the shark is a scary sight\n",
      "He swats the air with his power\n",
      "Once in a blue moon he is sure to come out\n",
      "Shark loves to fish to eat,\n",
      "One day, one day only, he may be big.\n",
      "\n",
      "\n",
      "Cubbie the Bear\n",
      "\n",
      "Cubbie used to eat crackers and honey\n",
      "And what happened to him was funny\n",
      "He was stung by fourteen foot long bees\n",
      "Now Cubbie eats broccoli and cheese\n",
      "\n",
      "\n",
      "Curly the Bear\n",
      "\n",
      "A bear so cute with people that is\n",
      "Curly the bear will creep up on you\n",
      "And if you're not sure, just peek behind the curtain\n",
      "\n",
      "At the top of an stairs, down below\n",
      "\n",
      "\n",
      "Daisy the Cowgirl\n",
      "\n",
      "Daisy drinks milk each night\n",
      "After having fun she's!\n",
      "Some say her milk teeth are as big as her hair\n",
      "Although Daisy really is!\n",
      "\n",
      "\n",
      "Derby the Horse\n",
      "\n",
      "Derby the horse is a man of few\n",
      "With little experience he crafts\n",
      "A robust and strong horse,\n",
      "All but built like a horse.\n",
      "\n",
      "\n",
      "Digger the Crab\n",
      "\n",
      "Digging in the sand doth prove\n",
      "That home is sweet and kind\n",
      "Travelling with a grin on my face\n",
      "Swinging my head in surprise, down the line\n",
      "\n",
      "\n",
      "Doby the Doberman\n",
      "\n",
      "This dog is little but he will not bow\n",
      "In the face of serious danger\n",
      "I barter his provisions for my bed\n",
      "Or he may stay for breakfast!\n",
      "\n",
      "\n",
      "Doodle the Rooster\n",
      "\n",
      "Listen closely to \"Wuthering Heights\"\n",
      "You will hear his cry\n",
      "We're giving him a heart transplant\n",
      "\n",
      "He will be forever known as \\\"The Rooster\\\"\n",
      "\n",
      "\n",
      "Dotty the Dalmation\n",
      "\n",
      "What are you doing\n",
      "Look at this puppy\n",
      "It's really adorable\n",
      "You will see Dotty doting\n",
      "Of all the dogs, this puppy is\n",
      "The one that doted on you.\n",
      "\n",
      "\n",
      "Vote for the Beaniest Bean in the Mountain\n",
      "Your vote will determine which Bean will get\n",
      "Bean Merino Green tea, or Bean Curigio\n",
      "Bean.\n",
      "\n",
      "\n",
      "Donald the Ugly Ghost\n",
      "\n",
      "Known by all in red and white\n",
      "We the people know who they are\n",
      "Always ready for anything\n",
      "While Donald is counting, the count gets\n",
      "As Donald digs, the bell goes.\n",
      "\n",
      "\n",
      "Vote Gary the Bad Guy\n",
      "\n",
      "Should you vote for Gary, I'd\n",
      "Your money, we have lots to give\n",
      "Give us a present, along the\n",
      "tiptoeing path we'll take you\n",
      "Take us to the ballot box, so we can vote\n",
      "On Election Day!\n",
      "\n",
      "\n",
      "Fetch the Buckaboo\n",
      "\n",
      "The dog that Fetch likes to flay\n",
      "He hides well in the sand and deep water\n",
      "At sea, in water slow and low\n",
      "Fetch is the dog that he claims to be.\n",
      "\n",
      "\n",
      "Forster the Pig\n",
      "\n",
      "اللهم مال النطل ین نیو بیر لیل \n",
      "الید لهم یله یل یل\n",
      "\n",
      "[510 | 428.00] loss=0.06 avg=0.62\n",
      "[520 | 435.64] loss=0.06 avg=0.60\n",
      "[530 | 443.29] loss=0.06 avg=0.59\n",
      "[540 | 450.91] loss=0.04 avg=0.58\n",
      "[550 | 458.54] loss=0.05 avg=0.57\n",
      "[560 | 466.17] loss=0.05 avg=0.55\n",
      "[570 | 473.81] loss=0.05 avg=0.54\n",
      "[580 | 481.43] loss=0.04 avg=0.53\n",
      "[590 | 489.06] loss=0.04 avg=0.52\n",
      "[600 | 496.69] loss=0.05 avg=0.51\n",
      "Saving checkpoint/run1/model-600\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name=model_name,\n",
    "              steps=600,\n",
    "              restore_from='fresh', # change to 'latest' to resume\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              learning_rate=1e-5,\n",
    "              sample_every=100,\n",
    "              save_every=200\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXSuTNERaw6K"
   },
   "source": [
    "## Notes on finetuning\n",
    "\n",
    "Keep an eye on the loss, and how quickly it is dropping. A too-rapid drop in loss could be a sign of overfitting, and a learning rate (lr) that is too high. \n",
    "\n",
    "After the model is trained, you can download the checkpoint folder to save your work. Training checkpoints are saved to `checkpoint/run1` (or whatever you chose for the run name above).\n",
    "\n",
    "You can compress it to a rar file and download that. Ask the instructor how.\n",
    "\n",
    "You're done! Feel free to go to the Generate Text From The Trained Model section to generate text based on your retrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune some more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already generated with gpt2, you need to reset the tf graph and gpt2 session. Otherwise, we create a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeXshJM-Cuaf",
    "outputId": "a3c75caa-917b-4818-ca2d-d78610d8b6f2"
   },
   "outputs": [],
   "source": [
    "#model_name = \"355M\" # same model as selected above\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# check if sess exists (e.g. if we ran section 1 above)\n",
    "var_exists = 'sess' in locals() or 'sess' in globals()\n",
    "\n",
    "if not var_exists:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune some more, run the following. Be sure to increase the number of steps (if it was `500` before, change to `1000` to train for 500 more. the number is cumulative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name=model_name,\n",
    "              steps=400,\n",
    "              restore_from='latest', # change to 'latest' to resume\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              learning_rate=1e-5,\n",
    "              sample_every=100,\n",
    "              save_every=100\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClJwpF_ACONp"
   },
   "source": [
    "# 3. Generate Text From The Finetuned Model\n",
    "\n",
    "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RNY6RBI9LmL",
    "outputId": "82574eaa-d39a-4665-b611-e5172848da57"
   },
   "outputs": [],
   "source": [
    "# gpt2.generate(sess, run_name='run1') # no prefix, unconditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.generate(sess, run_name='run1', prefix=\"ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF4-PqF0Fl7R"
   },
   "source": [
    "## Notes\n",
    "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
    "\n",
    "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
    "\n",
    "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
    "\n",
    "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
    "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DKMc0fiej4N",
    "outputId": "490a4648-d973-4675-cf9a-7a48c16fd736"
   },
   "outputs": [],
   "source": [
    "gpt2.generate(sess,\n",
    "              length=250,\n",
    "              temperature=0.7,\n",
    "              prefix=\"LORD\",\n",
    "              nsamples=5,\n",
    "              batch_size=5\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjjEN2Tafhl2"
   },
   "source": [
    "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
    "\n",
    "You can rerun the cells as many times as you want for even more generated texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fa6p6arifSL0"
   },
   "outputs": [],
   "source": [
    "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
    "\n",
    "gpt2.generate_to_file(sess,\n",
    "                      destination_path=gen_file,\n",
    "                      length=500,\n",
    "                      temperature=0.7,\n",
    "                      nsamples=100,\n",
    "                      batch_size=20\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-LRex8lfv1g"
   },
   "source": [
    "Download the file by hand in the browser at left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Trained Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTa6zf3e_9gV"
   },
   "source": [
    "Uploaded your saved checkpoint and unzip it.\n",
    "\n",
    "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
    "\n",
    "This will reset or start the tensorflow session as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fxL77nvAMAX",
    "outputId": "8938432a-3b86-4102-f32b-362721ecb897"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "if not sess:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)\n",
    "\n",
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ig-KVgkCDCKD"
   },
   "source": [
    "# Etcetera\n",
    "\n",
    "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIHiVP53FnsX"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmTXWNUygS5E"
   },
   "source": [
    "# License\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 Max Woolf\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- Max's [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
    "- Original repo: [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple) by [Max Woolf](http://minimaxir.com). \n",
    "- Original [google colab](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) from Max."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Train a GPT-2 Text-Generating Model w/ GPU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TensorFlow GPU 2.6 (py39)",
   "language": "python",
   "name": "tensorflow-gpu-2.6-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
